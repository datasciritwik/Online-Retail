{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "mDgbUHAGgjLW",
        "MSa1f5Uengrz",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "22aHeOlLveiV",
        "Yfr_Vlr8HBkt",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name** - Online Retail Customer Segmentation\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/datasciritwik/Online-Retail/main/Online-retail-shops-800x445.jpeg)"
      ],
      "metadata": {
        "id": "gMRWYywLNYRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This project analyzes a transactional dataset from a UK-based online retail company that specializes in unique all-occasion gifts, with a significant portion of its customer base being wholesalers. The dataset covers transactions from December 1, 2010, to December 9, 2011. The primary goal is to identify major customer segments using unsupervised machine learning techniques like clustering and association rules.**\n",
        "\n",
        "The project begins with data understanding and preparation. This involves exploring the dataset, handling missing values, removing duplicates, and addressing negative or return transactions. Data quality is improved by ensuring consistent product descriptions and converting data types for analysis.\n",
        "\n",
        "Exploratory data analysis reveals key insights about customer behavior and sales patterns. Visualizations like bar charts and pie charts are used to understand sales distribution across different countries, identify top customers by sales amount and event sales, and analyze the sales performance of top products.\n",
        "\n",
        "**The core of the project involves customer segmentation using the RFM model (Recency, Frequency, Monetary Value). This model helps understand customer behavior based on how recently they purchased, how often they purchase, and the total value of their purchases. Each RFM component is calculated and visualized to understand its distribution and identify potential outliers.**\n",
        "\n",
        "**Data preprocessing is performed to prepare the data for the K-means clustering algorithm. This includes log transformation and standardization to ensure that variables have a mean of 0 and a variance of 1, addressing the issue of varying ranges and potential outliers.**\n",
        "\n",
        "**The Elbow method is used to determine the optimal number of clusters for K-means. This involves analyzing the percentage of variance explained as a function of the number of clusters and identifying the point where the distortion begins to increase most rapidly.**\n",
        "\n",
        "**Silhouette analysis is then employed to evaluate the quality of the clustering results. This technique helps visualize the separation distance between clusters and identify potential issues like poorly separated clusters or misclassified data points.**\n",
        "\n",
        "The project leverages various Python libraries like Pandas, Scikit-learn, and Plotly to perform data manipulation, analysis, and visualization. The findings from this analysis can be used to develop targeted marketing campaigns, optimize product placement, and identify potential new customer segments.\n",
        "\n",
        "Further analysis could involve using association rule mining to identify relationships between products purchased together, leading to insights for product recommendations and targeted promotions. The project demonstrates a practical application of unsupervised machine learning techniques for customer segmentation and business decision-making in the online retail industry."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[GitHub Link](https://github.com/datasciritwik/Online-Retail.git)**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://pngimg.com/uploads/github/github_PNG23.png)"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Identify major customer segments on a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetching Dataset"
      ],
      "metadata": {
        "id": "qJuOGOshOQhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/datasciritwik/Online-Retail.git"
      ],
      "metadata": {
        "id": "bjMVIP3LOW2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing dependencies"
      ],
      "metadata": {
        "id": "9fVz0bduOe9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Orange3 pandasql Orange3-Associate -qq"
      ],
      "metadata": {
        "id": "h69fJKukOjDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "# Suppress warnings to keep the output clean\n",
        "import os\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)  # Ignore FutureWarnings\n",
        "warnings.filterwarnings('ignore')  # Ignore all warnings\n",
        "\n",
        "# Function to ignore warnings\n",
        "def ignore_warn(*args, **kwargs):\n",
        "    pass\n",
        "\n",
        "warnings.warn = ignore_warn  # Override the default warning function\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import datetime  # For handling date and time data\n",
        "import math  # For mathematical functions\n",
        "import numpy as np  # For numerical operations\n",
        "import matplotlib.pyplot as plt  # For static plotting\n",
        "import matplotlib.cm as cm  # For colormap handling\n",
        "\n",
        "# Enable inline plotting for Jupyter Notebooks\n",
        "%matplotlib inline\n",
        "\n",
        "# Import pandasql for SQL-like queries on DataFrames\n",
        "from pandasql import sqldf\n",
        "pysqldf = lambda q: sqldf(q, globals())  # Define a function to run SQL queries\n",
        "\n",
        "# Import seaborn for enhanced statistical data visualization\n",
        "import seaborn as sns\n",
        "sns.set(style=\"ticks\", color_codes=True, font_scale=1.5)  # Set seaborn style\n",
        "color = sns.color_palette()  # Get the default color palette\n",
        "sns.set_style('darkgrid')  # Set the style to dark grid\n",
        "\n",
        "# Import Plotly for interactive plotting\n",
        "import plotly as py\n",
        "import plotly.graph_objs as go\n",
        "import plotly.express as px\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "py.offline.init_notebook_mode()  # Initialize Plotly for offline use in Jupyter\n",
        "\n",
        "# Import statistical functions from SciPy\n",
        "from scipy import stats\n",
        "from scipy.stats import skew, norm, probplot, boxcox  # Specific statistical functions\n",
        "\n",
        "# Import preprocessing tools from Scikit-learn\n",
        "from sklearn import preprocessing  # For data preprocessing tasks\n",
        "\n",
        "# Import clustering algorithms and metrics from Scikit-learn\n",
        "from sklearn.cluster import KMeans  # KMeans clustering algorithm\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score  # Metrics for evaluating clustering\n",
        "\n",
        "# Import Orange for data mining and machine learning\n",
        "import Orange\n",
        "from Orange.data import Domain, DiscreteVariable, ContinuousVariable  # Data types for Orange\n",
        "from orangecontrib.associate.fpgrowth import *  # Association rule mining functions"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "data_path = \"/content/Online-Retail/Online Retail.xlsx\"\n",
        "df = pd.read_excel(data_path)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head().style.background_gradient(cmap='Blues')"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f'Number of Rows: {df.shape[0]}')\n",
        "print(f'Number of Columns: {df.shape[1]}')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "missing_values = df.isnull().sum()\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=missing_values.index, y=missing_values.values, palette=\"viridis\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Missing Values Count')"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset provided is a transactional dataset from a UK-based online retail company specializing in unique all-occasion gifts, with a significant portion of their customer base being wholesalers. The goal is to perform customer segmentation and gain insights into customer behavior and purchasing patterns.\n",
        "\n",
        "Customer segmentation is the process of dividing a company's customer base into distinct groups based on various characteristics such as demographics, purchase history, and behavior. The aim is to better understand customer needs and preferences to tailor marketing strategies, improve customer satisfaction, and increase revenue.\n",
        "\n",
        "|Rows|Columns|Columns with Missing Values|Duplicate Count|\n",
        "|-|-|-|-|\n",
        "|541909|8|Description(1454), CustomerID(135080)|5268|"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe().style.background_gradient(cmap='Blues')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include='object').T"
      ],
      "metadata": {
        "id": "dGT4HAmEWPo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **InvoiceNo**: A unique identifier for the invoice. An invoice number shared across rows means that those transactions were performed in a single invoice (multiple purchases).\n",
        "- **StockCode**: Identifier for items contained in an invoice.\n",
        "- **Description**: Textual description of each of the stock item.\n",
        "- **Quantity**: The quantity of the item purchased.\n",
        "- **InvoiceDate**: Date of purchase.\n",
        "- **UnitPrice**: Value of each item.\n",
        "- **CustomerID**: Identifier for customer making the purchase.\n",
        "- **Country**: Country of customer."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for col in df.columns:\n",
        "    print(f\"Feature Name: {col} \\n- Total Unique Values - {len(df[col].unique())}\\n\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Removing Rows with Null Values\n",
        "Since rows with null values contains `text values (Description)` and `Unique ID (CustomerID)` because of that we can't apply any kind of Imputation techniques."
      ],
      "metadata": {
        "id": "3RJyYU94ZV2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace = True, axis = 0)"
      ],
      "metadata": {
        "id": "i3FoHyrPZTKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Drop Duplicated"
      ],
      "metadata": {
        "id": "0MHjXwZQZfQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(inplace = True)"
      ],
      "metadata": {
        "id": "4HSBKHToZi2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking Remove negative or return transactions - Barplot"
      ],
      "metadata": {
        "id": "TFSJONWMZzPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate min and max values\n",
        "min_max = {}\n",
        "min_max['Quantity'] = [df['Quantity'].min(), df['Quantity'].max()]\n",
        "min_max['UnitPrice'] = [df['UnitPrice'].min(), df['UnitPrice'].max()]\n",
        "\n",
        "# Create a DataFrame for plotting\n",
        "min_max_df = pd.DataFrame(min_max, index=['Min', 'Max']).reset_index()\n",
        "min_max_df.rename(columns={'index': 'Metric'}, inplace=True)\n",
        "\n",
        "# Set the color palette\n",
        "palette = sns.color_palette(\"pastel\")\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Plot for Quantity\n",
        "sns.barplot(x='Metric', y='Quantity', data=min_max_df, ax=axes[0], color=palette[0], alpha=0.7)\n",
        "axes[0].set_title('Min-Max Chart for Quantity', fontsize=16)\n",
        "axes[0].set_xlabel('Metric', fontsize=14)\n",
        "axes[0].set_ylabel('Values', fontsize=14)\n",
        "axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "axes[0].axhline(0, color='red', linestyle='--', linewidth=1)  # Horizontal line at y=0\n",
        "\n",
        "# Annotate min and max values for Quantity\n",
        "for index, row in min_max_df.iterrows():\n",
        "    axes[0].text(row['Metric'], row['Quantity'] + 1, row['Quantity'],\n",
        "                  color='black', ha='center', fontsize=10)\n",
        "\n",
        "# Plot for Unit Price\n",
        "sns.barplot(x='Metric', y='UnitPrice', data=min_max_df, ax=axes[1], color=palette[1], alpha=0.7)\n",
        "axes[1].set_title('Min-Max Chart for Unit Price', fontsize=16)\n",
        "axes[1].set_xlabel('Metric', fontsize=14)\n",
        "axes[1].set_ylabel('Values', fontsize=14)\n",
        "axes[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "axes[1].axhline(0, color='red', linestyle='--', linewidth=1)  # Horizontal line at y=0\n",
        "\n",
        "# Annotate min and max values for Unit Price\n",
        "for index, row in min_max_df.iterrows():\n",
        "    axes[1].text(row['Metric'], row['UnitPrice'] + 0.1, row['UnitPrice'],\n",
        "                  color='black', ha='center', fontsize=10)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LP8KBRhVZ0Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Check if we had negative quantity and prices at same register:',\n",
        "     'No' if df[(df.Quantity<0) & (df.UnitPrice<0)].shape[0] == 0 else 'Yes', '\\n')\n",
        "print('Check how many register we have where quantity is negative',\n",
        "      'and prices is 0 or vice-versa:',\n",
        "      df[(df.Quantity<=0) & (df.UnitPrice<=0)].shape[0])\n",
        "print('\\nWhat is the customer ID of the registers above:',\n",
        "      df.loc[(df.Quantity<=0) & (df.UnitPrice<=0),\n",
        "                ['CustomerID']].CustomerID.unique())\n",
        "print('\\n% Negative Quantity: {:3.2%}'.format(df[(df.Quantity<0)].shape[0]/df.shape[0]))\n",
        "print('\\nAll register with negative quantity has Invoice start with:',\n",
        "      df.loc[(df.Quantity<0) & ~(df.CustomerID.isnull()), 'InvoiceNo'].apply(lambda x: x[0]).unique())\n",
        "print('\\nSee an example of negative quantity and others related records:')\n",
        "display(df[(df.CustomerID==12472) & (df.StockCode==22244)])"
      ],
      "metadata": {
        "id": "zLzBfqKEZ9Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Check register with UnitPrice negative:')\n",
        "display(df[(df.UnitPrice<0)])\n",
        "print(\"Sales records with Customer ID and zero in Unit Price:\",df[(df.UnitPrice==0)  & ~(df.CustomerID.isnull())].shape[0])\n",
        "df[(df.UnitPrice==0)  & ~(df.CustomerID.isnull())].head()"
      ],
      "metadata": {
        "id": "11AQXiAgaBKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, there are no records where quantity and price are negative, but there are 1.336 records where one of them is and the other is 0. However, note that for all these records we do not have the customer ID. So we conclude that we can erase all records in that quantity or the price and negative. **In addition, by the foregoing summary we see that there are 135,080 records without customer identification that we may also disregard.**"
      ],
      "metadata": {
        "id": "aRdE7Gu3aFv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Remove negative or return transactions"
      ],
      "metadata": {
        "id": "e-QJsfVLaLxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[~(df.Quantity<0)]\n",
        "df = df[df.UnitPrice>0]"
      ],
      "metadata": {
        "id": "VawUGZQZaOoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives the multiple descriptions for one of those items and we witness the simple ways in which data quality can be corrupted in any dataset. A simple spelling mistake can end up in reducing data quality and an erroneous analysis."
      ],
      "metadata": {
        "id": "zWTfsdwrab0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_desc = df[[\"StockCode\", \"Description\"]].groupby(by=[\"StockCode\"]).\\\n",
        "                apply(pd.DataFrame.mode).reset_index(drop=True)\n",
        "q = '''\n",
        "select df.InvoiceNo, df.StockCode, un.Description, df.Quantity, df.InvoiceDate,\n",
        "       df.UnitPrice, df.CustomerID, df.Country\n",
        "from df as df INNER JOIN\n",
        "     unique_desc as un on df.StockCode = un.StockCode\n",
        "'''\n",
        "\n",
        "df = pysqldf(q)"
      ],
      "metadata": {
        "id": "TLyZz0yrafAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Type Convertion and creating Amount column"
      ],
      "metadata": {
        "id": "VareUBuPbMp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.InvoiceDate = pd.to_datetime(df.InvoiceDate)\n",
        "df['Amount'] = df.Quantity*df.UnitPrice\n",
        "df.CustomerID = df.CustomerID.astype('Int64')"
      ],
      "metadata": {
        "id": "xzG-BmrwbQ2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code you shared performs the following data manipulations:\n",
        "\n",
        "- **Handles missing values:** Removes rows with missing values in any column.\n",
        "- **Removes duplicate rows:** Keeps only the unique rows in the dataset.\n",
        "- **Removes negative or return transactions:** Filters out transactions with negative quantity or unit price.\n",
        "- **Ensures consistent product descriptions:** Groups by StockCode and uses the mode to handle multiple descriptions for the same product.\n",
        "- **Performs type conversion:** Converts InvoiceDate to datetime, CustomerID to integer, and creates an Amount column by multiplying Quantity and UnitPrice.\n",
        "\n",
        "The insights from these manipulations include:\n",
        "\n",
        "- There were missing values in\n",
        "`Description` and `CustomerID` columns, which were removed as they can't be imputed with meaningful values.\n",
        "- There were duplicate transactions, which were removed to avoid inaccuracies in analysis.\n",
        "- Negative quantity values likely indicate returns, and were removed to focus on actual sales.\n",
        "- Some products had inconsistent descriptions, which were standardized to ensure data quality.\n",
        "- Type conversion ensures that data is in the correct format for analysis and modeling."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1. Normalized Amount Sales by Country (Bar Chart)"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Create a copy of the DataFrame\n",
        "temp1 = df.copy()\n",
        "\n",
        "# Normalize the 'Amount' values between 0 and 1\n",
        "temp1['normalized_amount'] = (temp1['Amount'] - temp1['Amount'].min()) / (temp1['Amount'].max() - temp1['Amount'].min())\n",
        "\n",
        "# Create the first figure for the bar chart\n",
        "fig1 = plt.figure(figsize=(12, 7))\n",
        "amount_sales = temp1.groupby([\"Country\"])['normalized_amount'].sum().sort_values(ascending=False)\n",
        "\n",
        "# Create the bar plot with thicker bars\n",
        "bars = amount_sales.plot(kind='bar', color='skyblue', ax=fig1.add_subplot(111), width=0.6)\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title('Normalized Amount Sales by Country', fontsize=20, fontweight='bold')\n",
        "plt.xlabel('Country', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Normalized Sales Amount', fontsize=12, fontweight='bold')\n",
        "plt.xticks(rotation=90, fontsize=10)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Adding data labels on top of the bars\n",
        "for index, value in enumerate(amount_sales):\n",
        "    plt.text(index, value + 0.02, f'{value:.4f}', ha='center', va='bottom', fontsize=8, color='black', rotation=90)\n",
        "\n",
        "# Show the first plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar chart effectively illustrates the distribution of sales amounts across various countries."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight from this chart is that the UK has the highest normalized sales amount, followed by Netherlands and EIRE. This suggests that these countries are the most important markets for the business."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights from this chart can definitely have a positive impact on the business. By identifying the top-performing countries, the business can develop targeted marketing campaigns and allocate resources effectively to maximize revenue. There's also potential for expanding into new markets or further developing existing high-performing ones. However, it's important to be mindful of potential downsides. If the business is heavily reliant on just a few countries, any negative changes in those markets could have a big impact. Also, the chart may highlight some underperforming countries, indicating a need for the business to reassess its strategies in those areas."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2. Internal Market Distribution (Pie Chart)"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Create the second figure for the pie chart\n",
        "fig2 = plt.figure(figsize=(6, 6))\n",
        "temp1['Internal'] = temp1.Country.apply(lambda x: 'UK' if x == 'United Kingdom' else 'Others')\n",
        "market = temp1.groupby([\"Internal\"]).Amount.sum().sort_values(ascending=False)\n",
        "\n",
        "# Create the pie chart\n",
        "colors = ['lightcoral', 'lightskyblue']\n",
        "plt.pie(market, labels=market.index, autopct='%1.1f%%', shadow=True, startangle=90, colors=colors)\n",
        "\n",
        "# Adding title and styling\n",
        "plt.title('Internal Market Distribution', fontsize=20, fontweight='bold')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "\n",
        "# Show the second plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart is a suitable choice because it effectively visualizes the proportion of sales attributed to the UK market compared to other countries. Pie charts are excellent for displaying the relative size of different categories within a whole, making it easy to understand the dominant market for the online retail business."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A significant portion of the sales comes from the UK, representing approximately 82.0% of the total sales. This indicates a strong domestic market presence for the online retail business. The remaining 18.0% represents sales from other countries, suggesting potential for international market expansion or a focus on strengthening the existing UK market dominance."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dominance of the UK market highlights the importance of continuing to cater to those customers, perhaps through tailored marketing or loyalty programs. At the same time, the presence of international sales, even if smaller, shows there's room to grow by expanding into new countries or improving the experience for existing international customers. However, there are also potential risks. Relying too much on the UK market could be problematic if that market experiences difficulties. Additionally, not focusing enough on expanding internationally might mean missing out on valuable opportunities for growth in the long run."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3. Top Customer Sales Contribution (Bar Chart)"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "# Create a figure for Top Customers Sales Amount\n",
        "fig1 = plt.figure(figsize=(25, 6))\n",
        "\n",
        "# Top Customers Sales Amount\n",
        "top_customers = df.groupby([\"CustomerID\"]).Amount.sum().sort_values(ascending=False)[:51]\n",
        "total_sales = df.groupby([\"CustomerID\"]).Amount.sum().sum()\n",
        "percent_sales = np.round((top_customers.sum() / total_sales) * 100, 2)\n",
        "\n",
        "# Bar chart for Top Customers\n",
        "ax1 = fig1.add_subplot(121)\n",
        "top_customers.plot(kind='bar', color='skyblue', ax=ax1)\n",
        "ax1.set_title(f'Top Customers: {percent_sales:.2f}% Sales Amount', fontsize=20, fontweight='bold')\n",
        "ax1.set_xlabel('Customer ID', fontsize=14)\n",
        "ax1.set_ylabel('Total Sales Amount', fontsize=14)\n",
        "ax1.tick_params(axis='x', rotation=90, labelsize=8)  # Decrease x tick font size\n",
        "ax1.tick_params(axis='y', labelsize=8)  # Decrease y tick font size\n",
        "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Show the first plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are great for comparing discrete categories, in this case, individual customers, and their corresponding sales amounts. This allows for easy identification of the highest-spending customers and understanding the distribution of sales among the top customer base."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It highlights the Pareto principle, often known as the 80/20 rule, where a small percentage of customers generate a large proportion of sales. This suggests that focusing on retaining and nurturing these high-value customers is crucial for business success. It also indicates potential for developing strategies to increase the spending of other customer segments."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By understanding which customers are the highest spenders, the business can implement tailored strategies like personalized offers and loyalty programs to keep them happy and coming back for more. It also allows for more targeted marketing efforts, ensuring that different customer groups receive the most relevant messages. However, there are potential drawbacks to consider. If the business relies too heavily on a small group of big spenders, losing even a few could have a significant impact. It's also important to avoid neglecting other customers who, with the right encouragement, could become more valuable over time. Finally, focusing solely on existing customers might lead to slower growth in the overall customer base."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4. Top Customer Sales Amount vs. Event Frequency (Bar Chart)"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Create a new figure for Top 10 Customers\n",
        "fig2 = plt.figure(figsize=(25, 7))\n",
        "f1 = fig2.add_subplot(121)\n",
        "\n",
        "# Top 10 Customers Sales Amount\n",
        "top_10_customers = df.groupby([\"CustomerID\"]).Amount.sum().sort_values(ascending=False)[:10]\n",
        "percent_sales_top_10 = np.round((top_10_customers.sum() / total_sales) * 100, 2)\n",
        "\n",
        "# Bar chart for Top 10 Customers\n",
        "top_10_customers.plot(kind='bar', color='lightgreen', ax=f1)\n",
        "f1.set_title(f'Top 10 Customers: {percent_sales_top_10:.2f}% Sales Amount', fontsize=20, fontweight='bold')\n",
        "f1.set_xlabel('Customer ID', fontsize=14)\n",
        "f1.set_ylabel('Total Sales Amount', fontsize=14)\n",
        "f1.tick_params(axis='x', rotation=45)\n",
        "f1.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Adding data labels on top of the bars\n",
        "for index, value in enumerate(top_10_customers):\n",
        "    f1.text(index, value + 5, f'{value}', ha='center', va='bottom', fontsize=12, color='black')\n",
        "\n",
        "# Top 10 Customers Event Sales\n",
        "f2 = fig2.add_subplot(122)\n",
        "top_10_event_sales = df.groupby([\"CustomerID\"]).Amount.count().sort_values(ascending=False)[:10]\n",
        "percent_sales_event = np.round((top_10_event_sales.sum() / df.groupby([\"CustomerID\"]).Amount.count().sum()) * 100, 2)\n",
        "\n",
        "# Bar chart for Top 10 Customers Event Sales\n",
        "top_10_event_sales.plot(kind='bar', color='salmon', ax=f2)\n",
        "f2.set_title(f'Top 10 Customers: {percent_sales_event:.2f}% Event Sales', fontsize=20, fontweight='bold')\n",
        "f2.set_xlabel('Customer ID', fontsize=14)\n",
        "f2.set_ylabel('Number of Events', fontsize=14)\n",
        "f2.tick_params(axis='x', rotation=45)\n",
        "f2.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Adding data labels on top of the bars\n",
        "for index, value in enumerate(top_10_event_sales):\n",
        "    f2.text(index, value + 1, f'{value}', ha='center', va='bottom', fontsize=12, color='black')\n",
        "\n",
        "# Show the second plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a useful approach for comparing related metrics for the same categories. In this case, it allows for a direct comparison of sales amount and the number of purchase events for the top 10 customers. This helps understand not only who the biggest spenders are but also how frequently they make purchases."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This provides a fascinating look at how different customers contribute to the business. We see that \"big spenders\" don't necessarily shop frequently, like customer 14646 who makes large, infrequent purchases. In contrast, customers like 18102 are very frequent shoppers, contributing to sales through the sheer volume of transactions rather than individual purchase size. This means that customer value can come in different forms, and understanding these variations is key to developing successful customer engagement strategies."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights from this can definitely guide the business towards a positive impact. By understanding that some customers are big spenders while others are frequent buyers, the business can create much more effective and personalized offers. This could involve anything from exclusive deals for high-value customers to loyalty programs for those who shop regularly. It also helps with things like making sure the right products are in stock and using the most effective communication channels for each customer type. However, there are some potential downsides to be aware of. Relying too much on any one type of customer could be risky if their behavior changes. It's also important to make sure all customer segments feel valued and appreciated, as even small, frequent purchases can add up significantly. The key takeaway is that a flexible approach to customer engagement, recognizing the different ways customers contribute value, is essential for sustained growth."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total sales amount and unique invoice counts\n",
        "AmoutSum = df.groupby([\"Description\"]).Amount.sum().sort_values(ascending=False)\n",
        "inv = df[[\"Description\", \"InvoiceNo\"]].groupby([\"Description\"]).InvoiceNo.unique().agg(np.size).sort_values(ascending=False)\n",
        "\n",
        "# Function to create bar charts\n",
        "def create_bar_chart(data, title, ax, annotate=True):\n",
        "    bars = data.plot(kind='bar', ax=ax, color='skyblue', edgecolor='black')\n",
        "    ax.set_title(title, fontsize=20, fontweight='bold')\n",
        "    ax.set_xlabel('Product Description', fontsize=14)\n",
        "    ax.set_ylabel('Sales Amount', fontsize=14)\n",
        "    ax.tick_params(axis='x', rotation=90, labelsize=10)  # Rotate labels to 90 degrees\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Adding data labels on top of the bars if annotate is True\n",
        "    if annotate:\n",
        "        for bar in bars.patches:\n",
        "            ax.annotate(f'{bar.get_height():.0f}',\n",
        "                        (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
        "                        ha='center', va='bottom', fontsize=10, color='black')"
      ],
      "metadata": {
        "id": "GQZdSJ64_Dan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5. Top Products by Sales Amount and Event Count (Bar Chart)"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Create the first figure for Top 10 Products\n",
        "fig1, (f1, f2) = plt.subplots(1, 2, figsize=(25, 7))\n",
        "\n",
        "Top10 = list(AmoutSum[:10].index)\n",
        "PercentSales = np.round((AmoutSum[Top10].sum() / AmoutSum.sum()) * 100, 2)\n",
        "PercentEvents = np.round((inv[Top10].sum() / inv.sum()) * 100, 2)\n",
        "create_bar_chart(AmoutSum[Top10],\n",
        "                  f'Top 10 Products in Sales Amount: {PercentSales:.2f}% of Amount and {PercentEvents:.2f}% of Events',\n",
        "                  f1)\n",
        "\n",
        "Top10Ev = list(inv[:10].index)\n",
        "PercentSales = np.round((AmoutSum[Top10Ev].sum() / AmoutSum.sum()) * 100, 2)\n",
        "PercentEvents = np.round((inv[Top10Ev].sum() / inv.sum()) * 100, 2)\n",
        "create_bar_chart(inv[Top10Ev],\n",
        "                  f'Events of Top 10 Most Sold Products: {PercentSales:.2f}% of Amount and {PercentEvents:.2f}% of Events',\n",
        "                  f2)\n",
        "\n",
        "# Show all plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this, which again uses two bar charts side-by-side, is designed to compare sales performance across different products. Bar charts excel at this type of comparison, allowing us to quickly identify the top-performing products in terms of both sales amount and the number of times they were sold (events). This dual perspective is crucial for understanding which products are driving the most revenue and which are most popular among customers."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This provides a valuable lesson in not judging a product by its sales figures alone. We can see that some products, like the \"REGENCY CAKESTAND 3 TIER,\" are incredibly popular and fly off the shelves, but don't necessarily bring in the big bucks. On the flip side, there are products that generate a lot of revenue despite being purchased less frequently, likely indicating a higher price point. This tells us that a product's success isn't just about how often it's sold, but also about the value of each sale. The business can use this information to develop different strategies for different products, perhaps focusing on volume for some and promoting higher-value items to specific customer segments."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The information from this can definitely help the business make smarter decisions and boost its success. By understanding how different products perform in terms of both popularity and revenue, the business can optimize its product mix, pricing, and promotions. This could involve anything from creating attractive bundle deals for popular items to targeting specific customer groups with high-value products. It also helps with things like managing stock levels effectively and getting ideas for new products that are likely to be a hit with customers. However, there are some potential pitfalls to avoid. For example, it's important to consider both sales figures and revenue generated, as focusing on just one aspect could lead to bad choices. The business also needs to make sure it's offering a good variety of products to cater to different needs and budgets. Finally, avoiding over-reliance on a few star products is important to ensure long-term stability and growth."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Create the second figure for Top 15 Products\n",
        "fig2 = plt.figure(figsize=(25, 7))\n",
        "Top15ev = list(inv[:15].index)\n",
        "PercentSales = np.round((AmoutSum[Top15ev].sum() / AmoutSum.sum()) * 100, 2)\n",
        "PercentEvents = np.round((inv[Top15ev].sum() / inv.sum()) * 100, 2)\n",
        "create_bar_chart(AmoutSum[Top15ev].sort_values(ascending=False),\n",
        "                  f'Sales Amount of Top 15 Most Sold Products: {PercentSales:.2f}% of Amount and {PercentEvents:.2f}% of Events',\n",
        "                  plt.gca())\n",
        "\n",
        "# Show all plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This utilizes a single bar chart to visualize the sales amount of the top 15 most sold products. While similar to Chart 5, this chart focuses specifically on the relationship between sales volume (represented by the frequency of products sold) and the resulting sales amount. A bar chart effectively displays this comparison, allowing for a clear understanding of which products generate the highest revenue despite not necessarily being the absolute top sellers in terms of quantity."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dives deeper into the sales performance of the top 15 most frequently sold products. The key insight here is that even among the most popular products, there's a significant variation in the generated sales amount. Some products that are frequently purchased contribute significantly to revenue, while others, despite their popularity, generate comparatively lower sales. This suggests a potential difference in price points or the purchase quantity of these products."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This provides a more granular view of product performance, showing that even among the most popular items, there's a difference in how much money they bring in. This information can be used to create smart strategies like bundling low-value items with others to increase overall sales or running targeted promotions to boost revenue from specific products. It also helps the business make better decisions about stock levels, ensuring they have enough of the right products on hand. However, it's important to avoid getting caught up in sales figures alone and to always consider the actual revenue generated by each product. Failing to do so could mean missing out on opportunities to increase profits, such as by not promoting or bundling popular items effectively."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Create the third figure for Top 50 Products\n",
        "fig3 = plt.figure(figsize=(25, 7))\n",
        "Top50 = list(AmoutSum[:50].index)\n",
        "PercentSales = np.round((AmoutSum[Top50].sum() / AmoutSum.sum()) * 100, 2)\n",
        "PercentEvents = np.round((inv[Top50].sum() / inv.sum()) * 100, 2)\n",
        "create_bar_chart(AmoutSum[Top50],\n",
        "                  f'Top 50 Products in Sales Amount: {PercentSales:.2f}% of Amount and {PercentEvents:.2f}% of Events',\n",
        "                  plt.gca(), annotate=False)\n",
        "# Show all plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart to display the top 50 products based on sales amount. This is an effective way to visualize the distribution of revenue across a larger set of products compared to charts 5 and 6, which focused on smaller subsets. The bar chart format allows for easy comparison of sales amounts for different products, highlighting those that contribute the most to overall revenue."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This expands the view of product performance by showcasing the top 50 products in terms of sales amount. This broader perspective reveals that a relatively small number of products contribute significantly to the overall revenue. This reinforces the Pareto principle observed in earlier charts, highlighting the importance of focusing on high-performing products and understanding their characteristics to inform future product development and marketing strategies."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us a big-picture view of the products that are really driving the business's financial success. By identifying these top performers, the business can make them a priority in terms of marketing, stock management, and even when developing new products. This focused approach can lead to higher sales and increased profits. It also helps to tailor marketing messages to the right customer groups who are most likely to be interested in these key products. However, it's important to remember that putting all your eggs in one basket can be risky. Over-relying on a few top products might leave the business vulnerable if those products lose popularity. It's also important to keep an eye on those niche products that might not be bestsellers but could still be attracting valuable customer segments."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Create the fourth figure for Top 50 Events\n",
        "fig4 = plt.figure(figsize=(25, 7))\n",
        "Top50Ev = list(inv[:50].index)\n",
        "PercentSales = np.round((AmoutSum[Top50Ev].sum() / AmoutSum.sum()) * 100, 2)\n",
        "PercentEvents = np.round((inv[Top50Ev].sum() / inv.sum()) * 100, 2)\n",
        "create_bar_chart(inv[Top50Ev],\n",
        "                  f'Top 50 Most Sold Products: {PercentSales:.2f}% of Amount and {PercentEvents:.2f}% of Events',\n",
        "                  plt.gca(), annotate=False)\n",
        "\n",
        "# Show all plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This employs a bar chart to visualize the top 50 most sold products based on the number of events or transactions. This complements Chart 7 by focusing on sales volume rather than just revenue. The bar chart format facilitates the comparison of transaction frequencies for different products, highlighting those that are most popular among customers regardless of their individual price or contribution to overall revenue."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shifts the focus to the top 50 products based purely on how often they are purchased. This highlights the products that are most popular among customers, regardless of their individual price or revenue contribution. This perspective helps understand customer preferences and identify potential high-demand products that might not necessarily be the top revenue generators."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us a clear picture of what customers love to buy, regardless of the price tag. This information is like gold for the business because it helps them understand customer preferences and tailor their offerings accordingly. It's also really useful for managing stock levels, making sure they have enough of the popular items to satisfy demand. Additionally, the business can use this knowledge to suggest related products or offer premium versions, potentially increasing the value of each sale. Popular products can even be used as bait in promotions to attract more customers. However, there are some potential downsides to watch out for. It's important to remember that popular doesn't always mean profitable. Focusing too much on high-volume sales might lead to overlooking those products that bring in more money despite being purchased less often. If most of the popular items have small profit margins, it could impact the overall profitability of the business."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Customer Segmentation:\n",
        "Customer segmentation is similarly the process of dividing an organizations customer bases into different sections or segments based on various customer attributes.  The process of customer segmentation is based on the premise of finding differences among the customers behavior and patterns.\n",
        "\n",
        "The major objectives and benefits behind the motivation for customer segmentation are:\n",
        "* **Higher Revenue**: This is the most obvious requirement of any customer segmentation project.\n",
        "* **Customer Understanding**: One of the mostly widely accepted business paradigms is know your customer and a segmentation of the customer base allows for a perfect dissection of this paradigm.\n",
        "* **Target Marketing**:  The most visible reason for customer segmentation is the ability to focus marketing efforts effectively and efficiently. If a firm knows the different segments of its customer base, it can devise better marketing campaigns which are tailor made for the segment. A good segmentation model allows for better understanding of customer requirements and hence increases the chances of the success of any marketing campaign developed by the organization.\n",
        "* **Optimal Product Placement**: A good customer segmentation strategy can also help the firm with developing or offering new products, or a bundle of products together as a combined offering.\n",
        "* **Finding Latent Customer Segments**: Finding out which segment of customers it might be missing to identifying untapped customer segments by focused on marketing campaigns or new business development.\n",
        "\n",
        "**Clustering**:\n",
        "\n",
        "The most obvious method to perform customer segmentation is using unsupervised Machine Learning methods like clustering.  The method is as simple as collecting as much data about the customers as possible in the form of features or attributes and then finding out the different clusters that can be obtained from that data. Finally, we can find traits of customer segments by analyzing the characteristics of the clusters.\n",
        "\n",
        "**Exploratory Data Analysis**:\n",
        "\n",
        "Using exploratory data analysis is another way of finding out customer segments. This is usually done by analysts who have a good knowledge about the domain relevant to both products and customers. It can be done flexibly to include the top decision points in an analysis.\n",
        "\n",
        "### RFM Model for Customer Value:\n",
        "\n",
        "Since our dataset is limited to the sales records, and didnt include anothers information about our customers, we will use a **RFM**,***Recency, Frequency and Monetary Value**, based model of customer value for finding our customer segments.\n",
        " The RFM model will take the transactions of a customer and calculate three important informational attributes about each customer:\n",
        "- **Recency**: The value of how recently a customer purchased at the establishment\n",
        "- **Frequency**: How frequent the customers transactions are at the establishment\n",
        "- **Monetary value**: The dollar (or pounds in our case) value of all the transactions that the customer made at the establishment\n",
        "\n",
        "#### Recency\n",
        "To create the recency feature variable, we need to decide the reference date for our analysis. Usually, we make use of the last transaction date plus one day. Then, we will construct the recency variable as the number of days before the reference date when a customer last made a purchase."
      ],
      "metadata": {
        "id": "69zNNjnZAip0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "refrence_date = df.InvoiceDate.max() + datetime.timedelta(days = 1)\n",
        "print('Reference Date:', refrence_date)\n",
        "df['days_since_last_purchase'] = (refrence_date - df.InvoiceDate).astype('timedelta64[s]')\n",
        "customer_history_df =  df[['CustomerID', 'days_since_last_purchase']].groupby(\"CustomerID\").min().reset_index()\n",
        "customer_history_df.rename(columns={'days_since_last_purchase':'recency'}, inplace=True)\n",
        "customer_history_df.describe().transpose()"
      ],
      "metadata": {
        "id": "BxbONz6kA0_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### We will plot the Recency Distribution and QQ-plot to identify substantive departures from normality, likes outliers, skewness and kurtosis."
      ],
      "metadata": {
        "id": "2xDWT5CCBCEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def QQ_plot(data, measure):\n",
        "    # Ensure the data is numeric and drop NaN values\n",
        "    data = pd.to_numeric(data, errors='coerce').dropna()\n",
        "\n",
        "    # Check if the data is empty after conversion\n",
        "    if data.empty:\n",
        "        print(\"The data provided is empty after conversion to numeric.\")\n",
        "        return\n",
        "\n",
        "    # Set the seaborn theme\n",
        "    sns.set_theme(style=\"whitegrid\")  # You can change the style to \"darkgrid\", \"white\", \"ticks\", etc.\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 7))\n",
        "\n",
        "    # Get the fitted parameters used by the function\n",
        "    (mu, sigma) = norm.fit(data)\n",
        "\n",
        "    # Kernel Density plot\n",
        "    fig1 = fig.add_subplot(121)\n",
        "    sns.histplot(data, kde=True, stat=\"density\", color='skyblue', bins=30, alpha=0.6)\n",
        "    x = np.linspace(min(data), max(data), 100)\n",
        "    p = norm.pdf(x, mu, sigma)\n",
        "    plt.plot(x, p, color='red', linewidth=2, label='Fitted Normal Distribution')\n",
        "    plt.legend()\n",
        "    fig1.set_title(f'{measure} Distribution (mu = {mu:.2f} and sigma = {sigma:.2f})', loc='center', fontsize=16)\n",
        "    fig1.set_xlabel(measure, fontsize=14)\n",
        "    fig1.set_ylabel('Density', fontsize=14)\n",
        "\n",
        "    # QQ plot\n",
        "    fig2 = fig.add_subplot(122)\n",
        "    res = probplot(data, dist=\"norm\", plot=fig2)\n",
        "\n",
        "    # Change the color of the QQ plot points and the reference line\n",
        "    fig2.get_lines()[0].set_color('orange')  # Change the color of the reference line\n",
        "    fig2.get_lines()[1].set_color('blue')  # Change the color of the points\n",
        "\n",
        "    fig2.set_title(f'{measure} Probability Plot (skewness: {data.skew():.6f} and kurtosis: {data.kurt():.6f})', loc='center', fontsize=16)\n",
        "\n",
        "    # Customize tick parameters\n",
        "    fig1.tick_params(axis='both', labelsize=12)\n",
        "    fig2.tick_params(axis='both', labelsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "b9x_AF1iBKZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "QQ_plot(customer_history_df.recency, 'Recency')"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two powerful tools to give us a deep dive into how recently customers have been making purchases. The histogram paints a picture of how the data is spread out, showing how many customers fall into different \"recency\" categories. The Q-Q plot, on the other hand, helps us figure out if this data follows a typical bell curve pattern or if it's skewed in some way. This combo is super helpful because it allows us to see things like whether the data is lopsided, has any extreme values, or any other quirks that could affect how we analyze and use this information for things like customer segmentation."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provides valuable insights into the recency of customer purchases. The histogram reveals that the data is heavily skewed to the right, indicating that a large proportion of customers have made purchases very recently. This suggests a potentially high level of customer engagement and repeat business. However, the long tail on the right indicates that there are also customers who haven't purchased in a while.\n",
        "\n",
        "The Q-Q plot further confirms the non-normality of the data, as the points deviate significantly from the straight line, particularly at the tails. This deviation suggests that the data might not be suitable for certain statistical analyses that assume normality. Transformations or alternative methods might be needed for accurate modeling and insights."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This provides valuable insights into customer purchase patterns, revealing that many customers are actively engaged while others haven't made a purchase recently. This knowledge allows the business to create targeted strategies, like sending personalized messages to those who haven't shopped in a while to win them back. It also helps with dividing customers into different groups based on their behavior, allowing for more effective marketing and tailored recommendations. Being aware of inactive customers helps prevent them from leaving and potentially losing valuable business. However, it's important to remember that the data isn't perfectly \"normal,\" meaning some adjustments might be needed when using it for analysis to avoid making wrong decisions based on skewed results. Ignoring inactive customers or misinterpreting the data could negatively impact the business."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis of Sales Recency Distribution**\n",
        "\n",
        "The first graph illustrates that the sales recency distribution is skewed, exhibiting a peak on the left and a long tail extending to the right. This indicates a deviation from normality, suggesting a positive skew.\n",
        "\n",
        "In the Probability Plot, we observe that the sales recency data does not align with the diagonal blue line, which represents a normal distribution. This further confirms the right skewness of the distribution.\n",
        "\n",
        "With a positive skewness of 1.25, we can affirm the lack of symmetry in the data, indicating that sales recency is skewed to the right. This means that the right tail is longer relative to the left tail. For reference, a normal distribution has a skewness of zero, and symmetric data should have a skewness close to zero, appearing identical on both sides of the center point.\n",
        "\n",
        "Kurtosis measures the heaviness of the tails in a distribution compared to a normal distribution. High kurtosis indicates heavy tails or the presence of outliers, while positive kurtosis suggests a heavy-tailed distribution, and negative kurtosis indicates a light-tailed distribution. In this case, with a kurtosis value of 0.43, the sales recency distribution is classified as heavy-tailed, suggesting the presence of some outliers."
      ],
      "metadata": {
        "id": "-i25mWEJBVNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "customer_freq = (df[['CustomerID', 'InvoiceNo']].groupby([\"CustomerID\", 'InvoiceNo']).count().reset_index()).\\\n",
        "                groupby([\"CustomerID\"]).count().reset_index()\n",
        "customer_freq.rename(columns={'InvoiceNo':'frequency'},inplace=True)\n",
        "customer_history_df = customer_history_df.merge(customer_freq)\n",
        "QQ_plot(customer_history_df.frequency, 'Frequency')"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dives into how often customers make purchases using a similar approach to Chart 9. The histogram gives us a visual snapshot of how often different customers shop, showing how many fall into various frequency categories. Meanwhile, the Q-Q plot helps us determine if this data follows a typical pattern or if it has any quirks that could skew the results of our analysis. This combination is really helpful for getting a complete picture of customer purchase frequency, allowing us to spot things like imbalances, outliers, or other unusual patterns that might need special attention when we use this data for things like customer segmentation or predicting future behavior."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This reveals interesting patterns in customer purchase frequency. The histogram shows a right-skewed distribution, indicating that a large portion of customers make purchases relatively infrequently. However, there's a long tail on the right, suggesting the presence of a smaller segment of customers who make purchases very frequently.\n",
        "\n",
        "The Q-Q plot confirms the non-normality of the data, with points deviating significantly from the straight line, especially at the tails. This deviation indicates that the frequency data might not be suitable for statistical analyses that assume normality. Transformations or alternative methods might be needed to ensure accurate insights and modeling."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This sheds light on how often customers shop, showing that while many make purchases infrequently, there's a valuable group of frequent buyers. This information is key for creating targeted marketing campaigns and loyalty programs that cater to the different spending habits of various customer groups. It also helps personalize product suggestions, ensuring that frequent shoppers are tempted with new items and less frequent ones are reminded of what they might be missing. Understanding purchase frequency also helps the business make smart decisions about stock levels. However, it's important to avoid focusing solely on the frequent shoppers and neglecting those who buy less often, as they still represent potential for increased sales. Also, remember that the data isn't perfectly \"normal,\" so some adjustments might be needed during analysis to avoid drawing misleading conclusions."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis of Sales Frequency Distribution**\n",
        "\n",
        "From the first graph, we observe that the sales frequency distribution is **skewed**, exhibiting a **peak** on the left with a long tail extending to the right. This indicates a **deviation from normal distribution** and suggests that the data is **positively biased**.\n",
        "\n",
        "In the **Probability Plot**, it is evident that the **sales frequency** does **not align with the diagonal line**, further confirming that the distribution is skewed to the right.\n",
        "\n",
        "With a ***skewness of 12.1***, we can affirm a **significant lack of symmetry** in the data. Additionally, a **kurtosis value of 249** indicates that the distribution is **heavy-tailed** and contains **outliers**.\n",
        "\n",
        "#### Monetary Value"
      ],
      "metadata": {
        "id": "Ibm5ZUppBeyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "customer_monetary_val = df[['CustomerID', 'Amount']].groupby(\"CustomerID\").sum().reset_index()\n",
        "customer_history_df = customer_history_df.merge(customer_monetary_val)\n",
        "QQ_plot(customer_history_df.Amount, 'Amount')"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This takes us on a journey into how much money customers are spending, using a similar approach to Charts 9 and 10. The histogram gives us a visual overview of customer spending, showing how many customers fall into different spending brackets. The Q-Q plot, on the other hand, helps us figure out if this data follows a typical pattern or if it has any unusual characteristics. This combo helps us get a complete picture of customer spending, allowing us to identify big spenders, those who spend less, and any unusual spending patterns that might need special attention when segmenting customers or making predictions about future spending."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This reveals insightful patterns in customer spending behavior. The histogram shows a right-skewed distribution, indicating that a large portion of customers spend relatively smaller amounts. However, a long tail on the right suggests the presence of a smaller segment of high-spending customers who contribute significantly to overall revenue.\n",
        "\n",
        "The Q-Q plot confirms the non-normality of the data, with points deviating considerably from the straight line, particularly at the tails. This deviation suggests that the monetary value data might not be suitable for statistical analyses that assume normality. Transformations or alternative methods might be needed to ensure accurate insights and modeling."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us a glimpse into the spending habits of customers, showing that while many spend moderate amounts, there's a valuable group of high rollers who contribute significantly to the bottom line. This information is crucial for tailoring marketing strategies and loyalty programs to different customer groups, ensuring that everyone feels valued and receives relevant offers. Understanding spending patterns allows the business to personalize recommendations, tempting big spenders with luxury items and offering budget-conscious shoppers attractive deals. By focusing on retaining those valuable high-spending customers, the business can boost its overall profitability. However, it's important to avoid alienating other customer segments by focusing solely on the high spenders. It's also crucial to remember that the data isn't perfectly \"normal,\" so adjustments might be needed during analysis to avoid making inaccurate conclusions."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis of Sales Amount Distribution**\n",
        "\n",
        "From the first graph, we observe that the sales amount distribution is **skewed**, exhibiting a **peak** on the left with a long tail extending to the right. This indicates a **deviation from normal distribution** and suggests that the data is **positively biased**.\n",
        "\n",
        "In the **Probability Plot**, it is evident that the **sales amount** does **not align with the diagonal line**, particularly on the right side.\n",
        "\n",
        "With a ***skewness of 19.3***, we confirm a **significant lack of symmetry** in the data. Additionally, a **kurtosis value of 478** indicates that the distribution is **extremely heavy-tailed** and contains **outliers**, with more than 10 being very extreme.\n",
        "\n",
        "Lets take a look at a statistical summary of this dataset:"
      ],
      "metadata": {
        "id": "b8nqbvuzCBnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_history_df.describe()"
      ],
      "metadata": {
        "id": "Zz2IXteVCHR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Preprocessing\n",
        "Once we have created our customer value dataset, we will perform some preprocessing on the data. For our clustering, we will be using the K-means clustering algorithm. One of the requirements for proper functioning of the algorithm is the mean centering of the variable values. Mean centering of a variable value means that we will replace the actual value of the variable with a standardized value, so that the variable has a mean of 0 and variance of 1. This ensures that all the variables are in the same range and the difference in ranges of values doesn't cause the algorithm to not perform well. This is akin to feature scaling.\n",
        "\n",
        "Another problem that you can investigate about is the huge range of values each variable can take. This\n",
        "problem is particularly noticeable for the monetary amount variable. To take care of this problem, we will transform all the variables on the log scale. This transformation, along with the standardization, will ensure that the input to our algorithm is a homogenous set of scaled and transformed values.\n",
        "\n",
        "An important point about the data preprocessing step is that sometimes we need it to be reversible. In our case, we will have the clustering results in terms of the log transformed and scaled variable. But to make inferences in terms of the original data, we will need to reverse transform all the variable so that we get back the actual RFM figures. This can be done by using the preprocessing capabilities of Python."
      ],
      "metadata": {
        "id": "qhRz5_9eCKT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_history_df['recency_log'] = customer_history_df['recency'].dt.days.apply(math.log)\n",
        "customer_history_df['frequency_log'] = customer_history_df['frequency'].apply(math.log)\n",
        "customer_history_df['amount_log'] = customer_history_df['Amount'].apply(math.log)\n",
        "feature_vector = ['amount_log', 'recency_log','frequency_log']\n",
        "X_subset = customer_history_df[feature_vector] #.as_matrix()\n",
        "scaler = preprocessing.StandardScaler().fit(X_subset)\n",
        "X_scaled = scaler.transform(X_subset)\n",
        "pd.DataFrame(X_scaled, columns=X_subset.columns).describe().T"
      ],
      "metadata": {
        "id": "vlEcM8srCU4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_history_df['recency'] = customer_history_df['recency'].dt.days"
      ],
      "metadata": {
        "id": "Wb4Bn2zqCa3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Create regression plots\n",
        "fig1 = plt.figure(figsize=(20, 14))\n",
        "\n",
        "# First subplot: Recency vs Amount\n",
        "ax1 = fig1.add_subplot(221)\n",
        "sns.regplot(x='recency', y='Amount', data=customer_history_df,\n",
        "            scatter_kws={'color': 'green', 's': 50},\n",
        "            line_kws={'color': 'orange', 'linewidth': 2},\n",
        "            ax=ax1)\n",
        "ax1.set_title('Recency vs Amount')\n",
        "\n",
        "# Second subplot: Frequency vs Amount\n",
        "ax2 = fig1.add_subplot(222)\n",
        "sns.regplot(x='frequency', y='Amount', data=customer_history_df,\n",
        "            scatter_kws={'color': 'green', 's': 50},\n",
        "            line_kws={'color': 'orange', 'linewidth': 2},\n",
        "            ax=ax2)\n",
        "ax2.set_title('Frequency vs Amount')\n",
        "\n",
        "# Third subplot: Log Recency vs Log Amount\n",
        "ax3 = fig1.add_subplot(223)\n",
        "sns.regplot(x='recency_log', y='amount_log', data=customer_history_df,\n",
        "            scatter_kws={'color': 'green', 's': 50},\n",
        "            line_kws={'color': 'orange', 'linewidth': 2},\n",
        "            ax=ax3)\n",
        "ax3.set_title('Log Recency vs Log Amount')\n",
        "\n",
        "# Fourth subplot: Log Frequency vs Log Amount\n",
        "ax4 = fig1.add_subplot(224)\n",
        "sns.regplot(x='frequency_log', y='amount_log', data=customer_history_df,\n",
        "            scatter_kws={'color': 'green', 's': 50},\n",
        "            line_kws={'color': 'orange', 'linewidth': 2},\n",
        "            ax=ax4)\n",
        "ax4.set_title('Log Frequency vs Log Amount')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scatter plot to visualize the relationship between \"Frequency\" (how often a customer makes a purchase) and \"MonetaryValue\" (the total amount spent by a customer). Scatter plots are excellent for visualizing the relationship between two continuous variables, allowing us to identify patterns, trends, and potential correlations.\n",
        "\n",
        "This visualization helps determine if there's a correlation between purchase frequency and monetary value, which can inform customer segmentation strategies and the development of targeted marketing campaigns. For instance, it can reveal if customers who purchase more frequently also tend to spend more or if there are other patterns in customer behavior."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This explores the connection between how often customers shop and how much they spend. It shows that while there's a general tendency for frequent buyers to spend more overall, it's not a hard and fast rule. This means that while loyal, frequent customers are often big spenders, there are also those who spend a lot even if they don't shop as often. Similarly, some customers shop frequently but might not spend as much on each purchase. This chart highlights that there are different types of valuable customers, and understanding these nuances is essential for creating marketing strategies that cater to their individual needs and spending habits."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This provides valuable insights for creating a more personalized and effective customer experience. By understanding the relationship between how often customers shop and how much they spend, the business can create specific groups of customers with similar behaviors. This allows for tailoring marketing campaigns and loyalty programs to resonate with each group's unique needs and preferences. For example, loyal customers who spend a lot might receive exclusive offers, while those who spend big but shop less often could get personalized recommendations based on their past purchases. Knowing these patterns also helps the business allocate resources wisely, ensuring that valuable customers receive special attention. However, it's important to remember that not all customers fit neatly into boxes. Overlooking certain groups or making assumptions based solely on the general trend could lead to missed opportunities and ineffective strategies."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "\n",
        "# Initialize Plotly for offline use\n",
        "init_notebook_mode(connected=True)\n",
        "\n",
        "# Create a 3D scatter plot\n",
        "fig = px.scatter_3d(customer_history_df,\n",
        "                     x='recency_log',\n",
        "                     y='frequency_log',\n",
        "                     z='amount_log',\n",
        "                     color='recency_log',  # You can choose a different column for color mapping\n",
        "                     title='3D Scatter Plot of Recency, Frequency, and Monetary Value',\n",
        "                     labels={'recency_log': 'Recency (Log)',\n",
        "                             'frequency_log': 'Frequency (Log)',\n",
        "                             'amount_log': 'Monetary (Log)'},\n",
        "                     opacity=0.6)\n",
        "\n",
        "# Update layout to set the size of the plot\n",
        "fig.update_layout(width=1000, height=600)\n",
        "\n",
        "fig.show(renderer=\"colab\")"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter plots created using Plotly are highly interactive and informative for visualizing the relationship between two variables. Plotly allows for customization of the plot with features like hover information, zooming, and panning, which enhance the understanding of data patterns and potential correlations.\n",
        "\n",
        "By using Plotly for this chart the business can explore the relationship between different variables in a more interactive and insightful way, enabling a deeper understanding of customer behavior and the effectiveness of segmentation strategies."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(10,5))\n",
        "corr = df.select_dtypes(exclude='object').iloc[:, 4:].corr()\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "sns.heatmap(corr, mask = mask, annot=True, cmap='flare')"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(10,5))\n",
        "corr = customer_history_df.select_dtypes(exclude='object').iloc[:, 4:].corr()\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "sns.heatmap(corr, mask = mask, annot=True, cmap='flare')"
      ],
      "metadata": {
        "id": "KIlzgs-nD0w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation heatmap is a powerful tool for visualizing the correlation between multiple variables. It uses a color-coded matrix to represent the correlation coefficients, making it easy to identify strong positive or negative relationships between variables."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation heatmap like Chart would reveal the relationships between different customer behavior metrics, likely including Recency, Frequency, and Monetary Value. Here are some possible insights you might find:\n",
        "\n",
        "- Strong Positive Correlation: A strong positive correlation between Frequency and MonetaryValue would indicate that customers who purchase more often tend to spend more overall. This is a common pattern in customer behavior.\n",
        "- Negative Correlation: A negative correlation between Recency and MonetaryValue could suggest that customers who haven't purchased recently tend to have lower overall spending. This could indicate a risk of churn.\n",
        "- Weak or No Correlation: If there's a weak or no correlation between certain variables, it suggests that those variables might not be strongly related to each other.\n",
        "By analyzing the correlation patterns in Chart, the business can:\n",
        "\n",
        "Identify key variables: Determine which variables are most strongly related to customer value and prioritize them for segmentation and marketing efforts.\n",
        "- Develop targeted strategies: Tailor marketing campaigns and customer engagement strategies based on the identified correlations. For example, focus on increasing purchase frequency for customers with high monetary value.\n",
        "- Improve predictive models: Use the correlation insights to select relevant features for predictive models like churn prediction or customer lifetime value estimation.\n"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df, diag_kind='kde', palette='set2')"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot is a powerful visualization tool that displays the relationships between all pairs of variables in a dataset. It creates a matrix of scatter plots, allowing you to quickly identify patterns, correlations, and potential outliers in the data. Pair plots are particularly useful for exploratory data analysis and understanding the interactions between multiple variables."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1. **The average amount spent by customers in the United Kingdom is higher than the average amount spent by customers from other countries.**"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Null Hypothesis (H0):** There is no difference in the average amount spent by customers in the United Kingdom and customers from other countries.\n",
        "\n",
        "- **Alternative Hypothesis (H1):** The average amount spent by customers in the United Kingdom is higher than the average amount spent by customers from other countries."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Create two groups for UK and other countries\n",
        "UK = df[df.Country == 'United Kingdom'].Amount\n",
        "Other_countries = df[df.Country != 'United Kingdom'].Amount"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the t-test\n",
        "from scipy import stats\n",
        "t_stat, p_value = stats.ttest_ind(UK, Other_countries)  # compare two independent samples\n",
        "print(f'T-statistic: {t_stat}, P-value: {p_value}')\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"- Reject the null hypothesis. \\nThere is a significant difference in the average amount spent between UK and other countries.\")\n",
        "else:\n",
        "    print(\"- Fail to reject the null hypothesis. \\nThere is no significant difference in the average amount spent between UK and other countries.\")"
      ],
      "metadata": {
        "id": "Uxv6Yl4eHtnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-test"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The t test estimates the true difference between two group means using the ratio of the difference in group means over the pooled standard error of both groups."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2. **The average Unit price by customers in the United Kingdom is higher than the average Unit price by customers from other countries.**"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Null Hypothesis (H0):** There is no difference in the average Unit Price by customers in the United Kingdom and customers from other countries.\n",
        "\n",
        "- **Alternative Hypothesis (H1):** The average amount spent by customers in the United Kingdom is higher than the average unit price by customers from other countries."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two groups for UK and other countries\n",
        "UK_unit = df[df.Country == 'United Kingdom'].UnitPrice\n",
        "Other_countries_unit = df[df.Country != 'United Kingdom'].UnitPrice"
      ],
      "metadata": {
        "id": "itTS5-MhlDNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "t_stat_unit, p_value_unit = stats.ttest_ind(UK_unit, Other_countries_unit)  # compare two independent samples\n",
        "print(f'T-statistic: {t_stat_unit}, P-value: {p_value_unit}')\n",
        "\n",
        "if p_value_unit < 0.05:\n",
        "    print(\"- Reject the null hypothesis. \\nThere is a significant difference in the average Unit Price between UK and other countries.\")\n",
        "else:\n",
        "    print(\"- Fail to reject the null hypothesis. \\nThere is no significant difference in the average Unit Price between UK and other countries.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-test"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The t test estimates the true difference between two group means using the ratio of the difference in group means over the pooled standard error of both groups."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3. **The average recency of customers who have purchased more than 50 different items is lower than the average recency of all customers.**"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "on6wbXfBE-Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Null Hypothesis (H0):** There is no difference in the average recency of customers who have purchased more than 50 different items.\n",
        "\n",
        "- **Alternative Hypothesis (H1):** There is difference in the average recency of customers who have purchased more than 50 different items."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two groups for UK and other countries\n",
        "recency_geter_50 = customer_history_df[customer_history_df.frequency > 50].recency\n",
        "recency_less_50 = customer_history_df[customer_history_df.frequency < 50].recency"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "t_stat_rency, p_value_rency = stats.ttest_ind(recency_geter_50, recency_less_50)  # compare two independent samples\n",
        "print(f'T-statistic: {t_stat_rency}, P-value: {p_value_rency}')\n",
        "\n",
        "if p_value_rency < 0.05:\n",
        "    print(\"- Reject the null hypothesis. \\nThere is a significant difference in the average Recency between recency more then 50 and recency less then 50.\")\n",
        "else:\n",
        "    print(\"- Fail to reject the null hypothesis. \\nThere is no significant difference in the average Recency between recency more then 50 and recency less then 50.\")"
      ],
      "metadata": {
        "id": "MzF_UbrcmzHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-test"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The t test estimates the true difference between two group means using the ratio of the difference in group means over the pooled standard error of both groups."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Unsupervised ML Model Implementation***"
      ],
      "metadata": {
        "id": "JWpXy3LrGSsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering and Segmentation"
      ],
      "metadata": {
        "id": "JHyoWiwjGhvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### k-Means Clustering"
      ],
      "metadata": {
        "id": "dRm0H8nCGqYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The K-means clustering belongs to the partition based\\centroid based hard clustering family of algorithms, a family of algorithms where each sample in a dataset is assigned to exactly one cluster.\n",
        "\n",
        "Based on this Euclidean distance metric, we can describe the k-means algorithm as a simple optimization problem, an iterative approach for minimizing the within-cluster sum of squared errors (SSE), which is sometimes also called cluster inertia. So, the objective of K-Means clustering is to minimize total intra-cluster variance, or, the squared error function:\n",
        "![image](https://www.saedsayad.com/images/Clustering_kmeans_c.png)\n",
        "\n",
        "The steps that happen in the K-means algorithm for partitioning the data are as given follows:\n",
        "1. The algorithm starts with random point initializations of the required number of centers. The K in K-means stands for the number of clusters.\n",
        "2. In the next step, each of the data point is assigned to the center closest to it. The distance metric used in K-means clustering is normal Euclidian distance.\n",
        "3. Once the data points are assigned, the centers are recalculated by averaging the dimensions of the points belonging to the cluster.\n",
        "4. The process is repeated with new centers until we reach a point where the assignments become stable. In this case, the algorithm terminates.\n",
        "\n",
        "##### K-means++\n",
        "- Place the initial centroids far away from each other via the k-means++ algorithm, which leads to better and more consistent results than the classic k-means.\n",
        "- To use k-means++ with scikit-learn's KMeans object, we just need to set the init parameter to k-means++ (the default setting) instead of random.\n",
        "\n",
        "#### The Elbow Method\n",
        "  \n",
        "Using the elbow method to find the optimal number of clusters. The idea behind the elbow method is to identify the value of k where the distortion begins to increase most rapidly. If k increases, the distortion will decrease, because the samples will be closer to the centroids they are assigned to.\n",
        "\n",
        "This method looks at the percentage of variance explained as a function of the number of clusters. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the \"elbow criterion\". This \"elbow\" cannot always be unambiguously identified.Percentage of variance explained is the ratio of the between-group variance to the total variance, also known as an F-test. A slight variation of this method plots the curvature of the within group variance."
      ],
      "metadata": {
        "id": "DDpoWuBHG0mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cl = 50\n",
        "corte = 0.1\n",
        "\n",
        "anterior = 100000000000000\n",
        "cost = []\n",
        "K_best = cl\n",
        "\n",
        "for k in range(1, cl + 1):\n",
        "    model = KMeans(\n",
        "        n_clusters=k,\n",
        "        init='k-means++',\n",
        "        n_init=10,\n",
        "        max_iter=300,\n",
        "        tol=1e-04,\n",
        "        random_state=101\n",
        "    )\n",
        "\n",
        "    model = model.fit(X_scaled)\n",
        "    labels = model.labels_\n",
        "    inertia = model.inertia_\n",
        "\n",
        "    if (K_best == cl) and (((anterior - inertia) / anterior) < corte):\n",
        "        K_best = k - 1\n",
        "\n",
        "    cost.append(inertia)\n",
        "    anterior = inertia\n",
        "\n",
        "# Inertia plot\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.plot(range(1, cl + 1), cost, marker='o', color='red', linestyle='-')\n",
        "plt.title('KMeans Inertia vs. Number of Clusters', fontsize=20, fontweight='bold')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.grid(True)\n",
        "plt.xticks(range(1, cl + 1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ffGyJPgjG7Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a KMeans model with the best K\n",
        "print('The best K suggested: ', K_best)\n",
        "model = KMeans(n_clusters=K_best, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=101)\n",
        "model = model.fit(X_scaled)\n",
        "labels = model.labels_\n",
        "\n",
        "# Visualization of clusters\n",
        "fig = plt.figure(figsize=(20, 10))\n",
        "\n",
        "# First subplot\n",
        "ax1 = fig.add_subplot(121)\n",
        "scatter1 = ax1.scatter(X_scaled[:, 1], X_scaled[:, 0], c=labels, cmap='viridis', s=50, alpha=0.6)\n",
        "ax1.set_xlabel(feature_vector[1])\n",
        "ax1.set_ylabel(feature_vector[0])\n",
        "ax1.set_title('Clusters Visualization (Feature 1 vs Feature 0)', fontsize=20, fontweight='bold')\n",
        "plt.colorbar(scatter1, ax=ax1, label='Cluster Label')\n",
        "\n",
        "# Second subplot\n",
        "ax2 = fig.add_subplot(122)\n",
        "scatter2 = ax2.scatter(X_scaled[:, 2], X_scaled[:, 0], c=labels, cmap='viridis', s=50, alpha=0.6)\n",
        "ax2.set_xlabel(feature_vector[2])\n",
        "ax2.set_ylabel(feature_vector[0])\n",
        "ax2.set_title('Clusters Visualization (Feature 2 vs Feature 0)', fontsize=20, fontweight='bold')\n",
        "plt.colorbar(scatter2, ax=ax2, label='Cluster Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A1eV0x1oG-io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that by the Elbow method from a K equal to 3 we already observed low rates of gain in the decay of the distortions with the decrease of K reaching the limit of 10% with the K equal to 7. With this in mind, we will begin to evaluate the options more deeply with 3, and 7, starting with the silhouette analysis."
      ],
      "metadata": {
        "id": "dgIU4BWJHJL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Silhouette analysis on K-Means clustering\n",
        "\n",
        "Silhouette analysis can be used to study the separation distance between the resulting clusters, as a strategy to quantifying the quality of clustering via graphical tool to plot a measure of how tightly grouped the samples in the clusters are. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually.\n",
        "\n",
        "It can also be applied to clustering algorithms other than k-means\n",
        "\n",
        "Silhouette coefficients has a range of \\[-1, 1\\], it calculated by:\n",
        "1. Calculate the cluster cohesion a( i )as the average distance between a sample x( i )   and all other points in the same cluster.\n",
        "2. Calculate the cluster separation b( i ) from the next closest cluster as the average distance between the sample x( i ) and all samples in the nearest cluster.\n",
        "3. Calculate the silhouette s( i )  as the difference between cluster cohesion and separation divided by the greater of the two, as shown here:\n",
        "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/3d80ab22fb291b347b2d9dc3cc7cd614f6b15479)\n",
        "Which can be also written as:\n",
        "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/ab5579a6c7150579af8a0d432b6630ba529376f0)\n",
        "\n",
        "Where:\n",
        "- If near +1, it indicate that the sample is far away from the neighboring clusters.\n",
        "- a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
        "- If most objects have a high value, then the clustering configuration is appropriate.\n",
        "- If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.\n",
        "- A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters\n",
        "- Negative values indicate that those samples might have been assigned to the wrong cluster.\n",
        "\n",
        "The silhouette plot can shows a bad K clusters pick for the given data due to the presence of clusters with below average silhouette scores and also due to wide fluctuations in the size of the silhouette plots. A good k clusters can found when all the plots are more or less of similar thickness and hence are of similar sizes.\n",
        "\n",
        "Although we have to keep in mind that in several cases and scenarios, sometimes we may have to drop the mathematical explanation given by the algorithm and look at the business relevance of the results obtained.\n",
        "\n",
        "Let's see below how our data perform for each K clusters groups (3, 5 and 7) in the silhouette score of each cluster, along with the center of each of the cluster discovered in the scatter plots, by amount_log vs recency_log and vs frequency_log."
      ],
      "metadata": {
        "id": "QV3vrzPPHMvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the seaborn style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "cluster_centers = dict()\n",
        "\n",
        "# Choose a color palette\n",
        "palette = sns.color_palette(\"husl\", 10)  # A color palette with 10 distinct colors\n",
        "\n",
        "for n_clusters in range(3, K_best + 1, 2):\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(25, 7))\n",
        "\n",
        "    # Set limits for silhouette plot\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    ax1.set_ylim([0, len(X_scaled) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Fit KMeans\n",
        "    clusterer = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=101)\n",
        "    cluster_labels = clusterer.fit_predict(X_scaled)\n",
        "\n",
        "    # Calculate silhouette score\n",
        "    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
        "    cluster_centers[n_clusters] = {\n",
        "        'cluster_center': clusterer.cluster_centers_,\n",
        "        'silhouette_score': silhouette_avg,\n",
        "        'labels': cluster_labels\n",
        "    }\n",
        "\n",
        "    # Silhouette values\n",
        "    sample_silhouette_values = silhouette_samples(X_scaled, cluster_labels)\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = palette[i % len(palette)]  # Use the color palette\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i), fontsize=12, color='black')\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"Silhouette Plot for {} Clusters\".format(n_clusters), fontsize=16)\n",
        "    ax1.set_xlabel(\"Silhouette Coefficient Values\", fontsize=14)\n",
        "    ax1.set_ylabel(\"Cluster Label\", fontsize=14)\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", label='Average Silhouette Score')\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_xticks(np.arange(-0.1, 1.1, 0.1))\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Scatter plot for first two features\n",
        "    centers = clusterer.cluster_centers_\n",
        "    colors = [palette[label % len(palette)] for label in cluster_labels]  # Use the color palette\n",
        "\n",
        "    # First scatter plot\n",
        "    x, y = 1, 0\n",
        "    ax2.scatter(X_scaled[:, x], X_scaled[:, y], marker='o', s=50, lw=0, alpha=0.6, c=colors, edgecolor='k')\n",
        "    ax2.scatter(centers[:, x], centers[:, y], marker='o', c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[x], c[y], marker='$%d$' % i, alpha=1, s=100, edgecolor='k', color='black')\n",
        "    ax2.set_title(\"{} Clustered Data (Feature 1 vs Feature 0)\".format(n_clusters), fontsize=16)\n",
        "    ax2.set_xlabel(feature_vector[x], fontsize=14)\n",
        "    ax2.set_ylabel(feature_vector[y], fontsize=14)\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Second scatter plot\n",
        "    x = 2\n",
        "    ax3.scatter(X_scaled[:, x], X_scaled[:, y], marker='o', s=50, lw=0, alpha=0.6, c=colors, edgecolor='k')\n",
        "    ax3.scatter(centers[:, x], centers[:, y], marker='o', c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "    for i, c in enumerate(centers):\n",
        "        ax3.scatter(c[x], c[y], marker='$%d$' % i, alpha=1, s=100, edgecolor='k', color='black')\n",
        "    ax3.set_title(\"Silhouette Score: {:1.2f}\".format(silhouette_avg), fontsize=16)\n",
        "    ax3.set_xlabel(feature_vector[x], fontsize=14)\n",
        "    ax3.set_ylabel(feature_vector[y], fontsize=14)\n",
        "    ax3.grid(True)\n",
        "\n",
        "    plt.suptitle(\"Silhouette Analysis for KMeans Clustering (n_clusters = {})\".format(n_clusters), fontsize=18, fontweight='bold')\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for the title\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DLNsi9VGHSlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we look at the results of the clustering process, we can infer some interesting insights:\n",
        "\n",
        "- First notice that all K clusters options is valid, because they don't have presence of clusters with below average silhouette scores.\n",
        "- In the other hand, all options had a some wide fluctuations in the size of the silhouette plots.\n",
        "\n",
        "So, the best choice may lie on the option that gives us a simpler business explanation and at the same time target customers in focus groups with sizes closer to the desired.\n",
        "\n",
        "#### Clusters Center:\n",
        "Let's look at the cluster center values after returning them to normal values from the log and scaled version."
      ],
      "metadata": {
        "id": "z6MAb00XHZO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['Amount',  'recency',  'frequency']\n",
        "for i in range(3,K_best+1,2):\n",
        "    print(\"for {} clusters the silhouette score is {:1.2f}\".format(i, cluster_centers[i]['silhouette_score']))\n",
        "    print(\"Centers of each cluster:\")\n",
        "    cent_transformed = scaler.inverse_transform(cluster_centers[i]['cluster_center'])\n",
        "    print(pd.DataFrame(np.exp(cent_transformed),columns=features))\n",
        "    print('-'*50)"
      ],
      "metadata": {
        "id": "XvA7P_prHdUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clusters Insights:\n",
        "\n",
        "With the plots and the center in the correct units, let's see some insights by each clusters groups:\n",
        "\n",
        "***In the three-cluster:***\n",
        "- The tree clusters appears have a good stark differences in the Monetary value of the customer, we will confirm this by a box plot.\n",
        "- Cluster 1 is the cluster of high value customer who shops frequently and is certainly an important segment for each business.\n",
        "- In the similar way we obtain customer groups with low and medium spends in clusters with labels 0 and 2, respectively.\n",
        "- Frequency and Recency correlate perfectly to the Monetary value based on the trend (High Monetary-Low Recency-High Frequency).\n",
        "\n",
        "***In the five-cluster:***\n",
        "- Note that clusters 0 and 1 are very similar to their cluster in the configuration with only 3 clusters.\n",
        "- The cluster 1 appears more robust on the affirmation of those who shop often and with high amount.\n",
        "- The cluster 2 are those who have a decent spend but are not as frequent as the cluster 1\n",
        "- The cluster 4 purchases medium amounts, with a relatively low frequency and not very recent\n",
        "- The cluster 3 makes low-cost purchases, with a relatively low frequency, but above 1, and made their last purchase more recently. This group of customers probably response to price discounts and can be subject to loyalty promotions to try increase the medium-ticket, strategy that can be better defined when we analyzing the market basket.\n",
        "- The silhouette score matrix says that the  five cluster segments are less optimal then the three cluster segments.\n",
        "\n",
        "***In the Seven-cluster:***\n",
        "- Definitely cluster 6 defines those who shop often and with high amount.\n",
        "- Clusters 1 and 5 show good spending and good frequency, only deferring in how recent were their last purchases, where 5 is older, which suggests an active action to sell to group 5 as soon as possible and another to 1 seeking to raise its frequency.\n",
        "- Cluster 0 presents the fourth best purchase and a reasonable frequency, but this is a long time without buying. This group should be sensible to promotions and activations, so that they do not get lost and make their next purchase.\n",
        "- Cluster 5 is similar to 0, but has made its purchases more recently and has a slightly better periodicity. Then actions must be taken to raise their frequency and reduce the chances of them migrating to cluster 0 by staying longer without purchasing products.\n",
        "\n",
        "#### Drill Down Clusters:\n",
        "\n",
        "To further drill down on this point and find out the quality of these difference, we can label our data with the corresponding cluster label and then visualize these differences. The following code will extract the clustering label and attach it with our customer summary dataset."
      ],
      "metadata": {
        "id": "OxWZCXMcHg03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming customer_history_df is already defined and contains the cluster labels\n",
        "customer_history_df['clusters_3'] = cluster_centers[3]['labels']\n",
        "customer_history_df['clusters_5'] = cluster_centers[5]['labels']\n",
        "customer_history_df['clusters_7'] = cluster_centers[7]['labels']\n",
        "display(customer_history_df.head())\n",
        "\n",
        "# Set the color palette\n",
        "palette = sns.color_palette(\"pastel\")\n",
        "\n",
        "# Create a figure with horizontal stacking\n",
        "fig, axs = plt.subplots(1, 3, figsize=(20, 7))\n",
        "\n",
        "# Plot for 3 clusters\n",
        "market_3 = customer_history_df.clusters_3.value_counts()\n",
        "axs[0].pie(market_3, labels=market_3.index, autopct='%1.1f%%', shadow=True, startangle=90, colors=palette, explode=[0.1]*len(market_3))\n",
        "axs[0].set_title('3 Clusters', fontsize=18, fontweight='bold')\n",
        "\n",
        "# Plot for 5 clusters\n",
        "market_5 = customer_history_df.clusters_5.value_counts()\n",
        "axs[1].pie(market_5, labels=market_5.index, autopct='%1.1f%%', shadow=True, startangle=90, colors=palette, explode=[0.1]*len(market_5))\n",
        "axs[1].set_title('5 Clusters', fontsize=18, fontweight='bold')\n",
        "\n",
        "# Plot for 7 clusters\n",
        "market_7 = customer_history_df.clusters_7.value_counts()\n",
        "axs[2].pie(market_7, labels=market_7.index, autopct='%1.1f%%', shadow=True, startangle=90, colors=palette, explode=[0.1]*len(market_7))\n",
        "axs[2].set_title('7 Clusters', fontsize=18, fontweight='bold')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WuDfDn6SHgbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have the labels assigned to each of the customers, our task is simple. Now we want to find out how the summary of customer in each group is varying. If we can visualize that information we will able to find out the differences in the clusters of customers and we can modify our strategy on the basis of those differences.\n",
        "\n",
        "The following code leverages plotly and will take the cluster labels we got for each configurations clusters and create boxplots. Plotly enables us to interact with the plots to see the central tendency values in each boxplot in the notebook. Note that we want to avoid the extremely high outlier values of each group, as they will interfere in making a good observation around the central tendencies of each cluster. Since we have only positive values, we will restrict the data such that only data points which are less than 0.95th percentile of the cluster is used. This will give us good information about the majority of the users in that cluster segment.\n",
        "\n",
        "I've used these charts to review my previously stated insights, but follow the same for you to explore:"
      ],
      "metadata": {
        "id": "ye78C8XqHoWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = ['Cluster 0', 'Cluster 1','Cluster 2','Cluster 3','Cluster 4', 'Cluster 5', 'Cluster 6']\n",
        "colors = ['rgba(93, 164, 214, 0.5)', 'rgba(255, 144, 14, 0.5)', 'rgba(44, 160, 101, 0.5)', 'rgba(255, 65, 54, 0.5)',\n",
        "          'rgba(22, 80, 57, 0.5)', 'rgba(127, 65, 14, 0.5)', 'rgba(207, 114, 255, 0.5)', 'rgba(127, 96, 0, 0.5)']\n",
        "cutoff_quantile = 95\n",
        "\n",
        "for n_clusters in range(3,K_best+1,2):\n",
        "    cl = 'clusters_' + str(n_clusters)\n",
        "    for fild in range(0, 3):\n",
        "        field_to_plot = features[fild]\n",
        "        y_data = list()\n",
        "        ymax = 0\n",
        "        for i in np.arange(0,n_clusters):\n",
        "            y0 = customer_history_df[customer_history_df[cl]==i][field_to_plot].values\n",
        "            y0 = y0[y0<np.percentile(y0, cutoff_quantile)]\n",
        "            if ymax < max(y0): ymax = max(y0)\n",
        "            y_data.insert(i, y0)\n",
        "\n",
        "        traces = []\n",
        "\n",
        "        for xd, yd, cls in zip(x_data[:n_clusters], y_data, colors[:n_clusters]):\n",
        "                traces.append(go.Box(y=yd, name=xd, boxpoints=False, jitter=0.5, whiskerwidth=0.2, fillcolor=cls,\n",
        "                    marker=dict( size=1, ),\n",
        "                    line=dict(width=1),\n",
        "                ))\n",
        "\n",
        "        layout = go.Layout(\n",
        "            title='Difference in {} with {} Clusters and {:1.2f} Score'.\\\n",
        "            format(field_to_plot, n_clusters, cluster_centers[n_clusters]['silhouette_score']),\n",
        "            yaxis=dict( autorange=True, showgrid=True, zeroline=True,\n",
        "                dtick = int(ymax/10),\n",
        "                gridcolor='black', gridwidth=0.1, zerolinecolor='rgb(255, 255, 255)', zerolinewidth=2, ),\n",
        "            margin=dict(l=40, r=30, b=50, t=50, ),\n",
        "            paper_bgcolor='white',\n",
        "            plot_bgcolor='white',\n",
        "            showlegend=False\n",
        "        )\n",
        "\n",
        "        fig = go.Figure(data=traces, layout=layout)\n",
        "        fig.show(renderer=\"colab\")"
      ],
      "metadata": {
        "id": "uhBmqruCHqYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next Steps in the Segmentation:\n",
        "\n",
        "To enhance discovery and can further improve the quality of clustering by adding relevant features, other customer information and purchases details may be included in this dataset.\n",
        "\n",
        "For example, but not limited to:\n",
        "- New indicators, such as customer relationship time, based on the date of your first purchase of the client\n",
        "- whether the customer is from abroad or not\n",
        "- some group or category of product to be obtained through the SKUs\n",
        "- External data vendors and use it, and so on.\n",
        "\n",
        "Another dimension to explore can be trying out different algorithms for performing the segmentation for instance hierarchical clustering, which we explored in some of the earlier chapters. A good segmentation process will encompass all these avenues to arrive at optimal segments that provide valuable insight.\n",
        "\n",
        "## Cross Selling\n",
        "\n",
        "The cross selling is the ability to sell more products to a customer by analyzing the customer's shopping trends as well as general shopping trends and patterns which are in common with the customer's shopping patterns. More often than not, these recommended products would be very appealing. The retailer will often offer you a bundle of products with some attractive offer and it is highly likely that we will end up buying the bundled products instead of just the original item.\n",
        "\n",
        "So, we research the customer transactions and find out potential additions to the customer's original needs and offer it to the customer as a suggestion in the hope and intent that they buy them benefiting both the customer as well as the retail establishment.\n",
        "\n",
        "In this section, we explore association rule-mining, a powerful technique that can be used for cross selling, then we apply the concept of market basket analysis to our retail transactions dataset.\n",
        "\n",
        "### Market Basket Analysis with Association Rule-Mining\n",
        "![image](https://fiverr-res.cloudinary.com/images/t_main1,q_auto,f_auto/gigs/117717078/original/7cd5947def9d84483612ef9b19829323850ac9af/bulid-a-product-recommender-system-for-ur-site.jpg)\n",
        "The whole concept of association rule-mining is based on the concept that customer purchase behavior has a pattern which can be exploited for selling more items to the customer in the future.\n",
        "\n",
        "Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness. This rule-based approach also generates new rules as it analyzes more data. The ultimate goal, assuming a large enough dataset, is to help a machine mimic the human brain's feature extraction and abstract association capabilities from new uncategorized data.\n",
        "\n",
        "An association rule usually has the structure like below:\n",
        "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/3141048979b977982202dbf7a80596f8a6b1177e)\n",
        "This rule can be read in the obvious manner that when the customer bought items on the left of the rule he is likely to buy the item on the right. Following are some vital concepts pertaining to association rule-mining.\n",
        "- **Itemset**: Is just a collection of one or more items that occur together in a transaction. For example, here {milk, bread} is example of an itemset.\n",
        "- **Support**: is defined as number of times an itemset appears in the dataset. The support of ***X*** with respect to ***T*** is defined as the proportion of transactions ***t*** in the dataset which contains the itemset ***X***. Mathematically it is defined as:\n",
        "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/1c6acacd3b17051205704b5d323c83fc737e5db1)\n",
        "- **Confidence**: Confidence is an indication of how often the rule has been found to be true. It is a measure of the times the number of times a rule is found to exist in the dataset. For a rule which states { beer -> diaper } the confidence is mathematically defined as:\n",
        "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/90324dedc399441696116eed3658fd17c5da4329)\n",
        "- **Lift**: Lift of the rule is defined as the ratio of observed support to the support expected in the case the elements of the rule were independent. For the previous set of transactions if the rule is defined as { X -> Y }, then the lift of the rule is defined as:\n",
        "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/c392e3111167b60687405dfdc7ed55f22409f4c5)\n",
        "    - If the rule had a lift of 1, it would imply that the probability of occurrence of the antecedent and that of the consequent are independent of each other. When two events are independent of each other, no rule can be drawn involving those two events.\n",
        "    - If the lift is > 1, that lets us know the degree to which those two occurrences are dependent on one another, and makes those rules potentially useful for predicting the consequent in future data sets.\n",
        "    - If the lift is < 1, that lets us know the items are substitute to each other. This means that presence of one item has negative effect on presence of other item and vice versa.\n",
        "- **Frequent itemset**: Frequent itemsets are itemsets whose support is greater than a user defined support threshold.\n",
        "- **Conviction**: Is the ratio of the expected frequency that item X occurs without a item Y (that is to say, the frequency that the rule makes an incorrect prediction) if X and Y were independent divided by the observed frequency of incorrect predictions. The conviction of a rule is defined as:\n",
        "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/4c2228820d6a8cb5a84bd059d53764a6b9280386)\n",
        "\n",
        "#### Algorithms:\n",
        "\n",
        "Some well-known algorithms are Apriori, Eclat and FP-Growth, but they only do half the job, since they are algorithms for mining frequent itemsets. Another step needs to be done after to generate rules from frequent itemsets found in a database.\n",
        "\n",
        "The major bottleneck in any association rule-mining algorithm is the generation of frequent itemsets. If the transaction\n",
        "dataset is having k unique products, then potentially we have 2<sup>k</sup> possible itemsets.\n",
        "\n",
        "##### Apriori\n",
        "Apriori uses a breadth-first search strategy to count the support of itemsets and uses a candidate generation function which exploits the downward closure property of support. So, the algorithm will first generate these itemsets and then proceed to finding the frequent itemsets.  For around 100 unique products the possible number of itemsets is huge, and shows up that the Apriori algorithm prohibitively computationally expensive.\n",
        "\n",
        "##### Eclat algorithm\n",
        "Eclat is a depth-first search algorithm based on set intersection. It is suitable for both sequential as well as parallel execution with locality-enhancing properties.\n",
        "\n",
        "##### FP Growth\n",
        "FP stands for frequent pattern. The FP growth algorithm is superior to Apriori algorithm as it doesn't need to generate all the candidate itemsets. The FP growth algorithm uses a divide-and-conquer strategy and leverages a special data structure called the FP-tree, to find frequent itemsets without generating all itemsets. The core steps of the algorithm are as follows:\n",
        "1. In the first pass, the algorithm take in the transactional database and counts occurrence of items (attribute-value pairs) in the dataset, and stores them to 'header table'.\n",
        "2. In the second pass, it builds the FP-tree structure by inserting instances to represent frequent itemsets. Items in each instance have to be sorted by descending order of their frequency in the dataset, so that the tree can be processed quickly. Items in each instance that do not meet minimum coverage threshold are discarded. If many instances share most frequent items, FP-tree provides high compression close to tree root.\n",
        "3. Divide this compressed representation into multiple conditional datasets such that each one is associated with a frequent pattern.\n",
        "4. Mine for patterns in each such dataset so that shorter patterns can be recursively concatenated to longer patterns, hence making it more efficient.\n",
        "\n",
        "Recursive processing of this compressed version of main dataset grows large item sets directly, instead of generating candidate items and testing them against the entire database. Growth starts from the bottom of the header table (having longest branches), by finding all instances matching given condition. New tree is created, with counts projected from the original tree corresponding to the set of instances that are conditional on the attribute, with each node getting sum of its children counts. Recursive growth ends when no individual items conditional on the attribute meet minimum support threshold, and processing continues on the remaining header items of the original FP-tree.\n",
        "\n",
        "Once the recursive process has completed, all large item sets with minimum coverage have been found, and association rule creation begins.\n",
        "\n",
        "#### Build Transaction Dataset\n",
        "In order to perform our data in these algorithms, we must first turn them into a sales event table where each product sold will be represented by a column, having its value 1 for when it was sold in that event or zero when not. This will generate a sparse table"
      ],
      "metadata": {
        "id": "PwSBVUA0HwLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "items = list(df.Description.unique())\n",
        "grouped = df.groupby('InvoiceNo')\n",
        "transaction_level = grouped.aggregate(lambda x: tuple(x)).reset_index()[['InvoiceNo','Description']]\n",
        "transaction_dict = {item:0 for item in items}\n",
        "output_dict = dict()\n",
        "temp = dict()\n",
        "for rec in transaction_level.to_dict('records'):\n",
        "    invoice_num = rec['InvoiceNo']\n",
        "    items_list = rec['Description']\n",
        "    transaction_dict = {item:0 for item in items}\n",
        "    transaction_dict.update({item:1 for item in items if item in items_list})\n",
        "    temp.update({invoice_num:transaction_dict})\n",
        "\n",
        "new = [v for k,v in temp.items()]\n",
        "transaction_df = pd.DataFrame(new)"
      ],
      "metadata": {
        "id": "XnjjEHi_H3P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prune Dataset for frequently purchased items\n",
        "We saw in the earlier on EDA how only a handful of items are responsible for bulk of our sales so we want to prune our dataset to reflect this information. For this we have created a function prune_dataset below, which will help us reduce the size of our dataset based on our requirements. The function can be used for performing three types of pruning:\n",
        "- Pruning based on percentage of total sales: The parameter total_sales_perc will help us select the number of items that will explain the required percentage of sales. The default value is 50% or 0.5.\n",
        "- Pruning based on ranks of items: Another way to perform the pruning is to specify the starting and the ending rank of the items for which we want to prune our dataset.\n",
        "- Pruning based on list of features passed to the parameter TopCols.\n",
        "\n",
        "By default, we will only look for transactions which have at least two items, as transactions with only one item are counter to the whole concept of association rule-mining."
      ],
      "metadata": {
        "id": "qLZDk1R3H6C3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_dataset(input_df, length_trans = 2, total_sales_perc = 0.5,\n",
        "                  start_item = None, end_item = None, TopCols = None):\n",
        "    if 'total_items' in input_df.columns:\n",
        "        del(input_df['total_items'])\n",
        "    item_count = input_df.sum().sort_values(ascending = False).reset_index()\n",
        "    total_items = sum(input_df.sum().sort_values(ascending = False))\n",
        "    item_count.rename(columns={item_count.columns[0]:'item_name',\n",
        "                               item_count.columns[1]:'item_count'}, inplace=True)\n",
        "    if TopCols:\n",
        "        input_df['total_items'] = input_df[TopCols].sum(axis = 1)\n",
        "        input_df = input_df[input_df.total_items >= length_trans]\n",
        "        del(input_df['total_items'])\n",
        "        return input_df[TopCols], item_count[item_count.item_name.isin(TopCols)]\n",
        "    elif end_item > start_item:\n",
        "        selected_items = list(item_count[start_item:end_item].item_name)\n",
        "        input_df['total_items'] = input_df[selected_items].sum(axis = 1)\n",
        "        input_df = input_df[input_df.total_items >= length_trans]\n",
        "        del(input_df['total_items'])\n",
        "        return input_df[selected_items],item_count[start_item:end_item]\n",
        "    else:\n",
        "        item_count['item_perc'] = item_count['item_count']/total_items\n",
        "        item_count['total_perc'] = item_count.item_perc.cumsum()\n",
        "        selected_items = list(item_count[item_count.total_perc < total_sales_perc].item_name)\n",
        "        input_df['total_items'] = input_df[selected_items].sum(axis = 1)\n",
        "        input_df = input_df[input_df.total_items >= length_trans]\n",
        "        del(input_df['total_items'])\n",
        "        return input_df[selected_items], item_count[item_count.total_perc < total_sales_perc]"
      ],
      "metadata": {
        "id": "dkHzGP2XH8OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_df, item_counts = prune_dataset(input_df=transaction_df, length_trans=2,start_item = 0, end_item = 15)\n",
        "print('Total of Sales Amount by the Top 15 Products in Sales Events (Invoice): {:.2f}'.format(AmoutSum[Top15ev].sum()))\n",
        "print('Number of Sales Events:', output_df.shape[0])\n",
        "print('Number of Products:', output_df.shape[1])"
      ],
      "metadata": {
        "id": "ZAjIeHJYH-R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the aesthetic style of the plots\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a count plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='item_count', y='item_name', data=item_counts, palette='gnuplot2')\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Item Count Distribution', fontsize=16)\n",
        "plt.xlabel('Count', fontsize=14)\n",
        "plt.ylabel('Item Name', fontsize=14)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tjYN25uKIAyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we find out that we have 15 items responsible for 8,73% of sales amount and close to 5% of the events result in 4.664 transactions that have those items along with other items. The next step is to convert this selected data into the required table data structure.\n",
        "\n",
        "#### Association Rule Mining with FP Growth\n",
        "\n",
        "##### Orange Table Data Structure\n",
        "Since we are using the Orage framework we still have to convert it to the Table data structure by providing the metadata about our columns. We need to define the domain for each of our variables. The domain means the possible set of values that each of our variables can use. This information will be stored as metadata and will be used in later transformation of the data. As our columns are only having binary values,we can easily create the domain by using this information."
      ],
      "metadata": {
        "id": "U6opOsOtIEXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_assoc_rules = output_df\n",
        "# Defined the data domain by specifying each variable as a DiscreteVariable having values as (0, 1)\n",
        "domain_transac = Domain([DiscreteVariable.make(name=item,values=['0', '1']) \\\n",
        "                         for item in input_assoc_rules.columns])\n",
        "\n",
        "# Then using this domain, we created our Table structure for our data\n",
        "# Use .values instead of .as_matrix()\n",
        "data_tran = Orange.data.Table.from_numpy(domain=domain_transac,\n",
        "                                         X=input_assoc_rules.values, Y= None)\n",
        "\n",
        "# Coding our input so that the entire domain is represented as binary variables\n",
        "data_tran_en, mapping = OneHot.encode(data_tran, include_class=True)"
      ],
      "metadata": {
        "id": "IJ4Xw4ooIPzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "support = 0.01\n",
        "print(\"num of required transactions = \", int(input_assoc_rules.shape[0]*support))\n",
        "num_trans = input_assoc_rules.shape[0]*support\n",
        "itemsets = dict(frequent_itemsets(data_tran_en, support))\n",
        "print('Items Set Size:', len(itemsets))"
      ],
      "metadata": {
        "id": "jtjEfyqbIYrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we get a whopping 663,273 itemsets for only 15 itens and a support of only 1%! This will increase exponentially if we decrease the support or if we increase the number of items in our dataset. The next step is specifying a confidence value and generating our rules. The following code snippet will perform rule generation and decoding of rules, and then compile it all in a neat dataframe that we can use for further analysis."
      ],
      "metadata": {
        "id": "9gW9cHNjIa_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confidence = 0.6\n",
        "rules_df = pd.DataFrame()\n",
        "if len(itemsets) < 1000000:\n",
        "    rules = [(P, Q, supp, conf)\n",
        "    for P, Q, supp, conf in association_rules(itemsets, confidence)\n",
        "       if len(Q) == 1 ]\n",
        "\n",
        "    names = {item: '{}={}'.format(var.name, val)\n",
        "        for item, var, val in OneHot.decode(mapping, data_tran, mapping)}\n",
        "\n",
        "    eligible_ante = [v for k,v in names.items() if v.endswith(\"1\")]\n",
        "\n",
        "    N = input_assoc_rules.shape[0]\n",
        "\n",
        "    rule_stats = list(rules_stats(rules, itemsets, N))\n",
        "\n",
        "    rule_list_df = []\n",
        "    for ex_rule_frm_rule_stat in rule_stats:\n",
        "        ante = ex_rule_frm_rule_stat[0]\n",
        "        cons = ex_rule_frm_rule_stat[1]\n",
        "        named_cons = names[next(iter(cons))]\n",
        "        if named_cons in eligible_ante:\n",
        "            rule_lhs = [names[i][:-2] for i in ante if names[i] in eligible_ante]\n",
        "            ante_rule = ', '.join(rule_lhs)\n",
        "            if ante_rule and len(rule_lhs)>1 :\n",
        "                rule_dict = {'support' : ex_rule_frm_rule_stat[2],\n",
        "                             'confidence' : ex_rule_frm_rule_stat[3],\n",
        "                             'coverage' : ex_rule_frm_rule_stat[4],\n",
        "                             'strength' : ex_rule_frm_rule_stat[5],\n",
        "                             'lift' : ex_rule_frm_rule_stat[6],\n",
        "                             'leverage' : ex_rule_frm_rule_stat[7],\n",
        "                             'antecedent': ante_rule,\n",
        "                             'consequent':named_cons[:-2] }\n",
        "                rule_list_df.append(rule_dict)\n",
        "    rules_df = pd.DataFrame(rule_list_df)\n",
        "    print(\"Raw rules data frame of {} rules generated\".format(rules_df.shape[0]))\n",
        "    if not rules_df.empty:\n",
        "        pruned_rules_df = rules_df.groupby(['antecedent','consequent']).max().reset_index()\n",
        "    else:\n",
        "        print(\"Unable to generate any rule\")"
      ],
      "metadata": {
        "id": "-GYK76AOIdDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explore The Association Rule Created\n",
        "\n",
        "Let's see what we get in the first 5 rules with highest confidence:"
      ],
      "metadata": {
        "id": "C_O3loHdIlQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dw = pd.options.display.max_colwidth\n",
        "pd.options.display.max_colwidth = 100\n",
        "(rules_df[['consequent', 'antecedent', 'support','confidence','lift']].\\\n",
        " groupby(['consequent', 'antecedent']).first()\n",
        "                                      .reset_index()\n",
        "                                      .sort_values(['confidence', 'support', 'lift'],\n",
        "                                                   ascending=False)).head()"
      ],
      "metadata": {
        "id": "pK-A7oEHIqaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(rules_df[['consequent', 'antecedent', 'support','confidence','lift']].\\\n",
        " groupby(['consequent', 'antecedent']).first()\n",
        "                                      .reset_index()\n",
        "                                      .sort_values(['support', 'confidence', 'lift'],\n",
        "                                                   ascending=False)).head()"
      ],
      "metadata": {
        "id": "lhuuY4F-I201"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the DataFrame\n",
        "top_rules = (rules_df[['consequent', 'antecedent', 'support','confidence','lift']].\\\n",
        " groupby(['consequent', 'antecedent']).first()\n",
        "                                      .reset_index()\n",
        "                                      .sort_values(['support', 'confidence', 'lift'],\n",
        "                                                   ascending=False)).head(10)\n",
        "\n",
        "# Reset the display option\n",
        "pd.options.display.max_colwidth = dw\n",
        "\n",
        "# Create a mapping for the labels\n",
        "labels = list(pd.concat([top_rules['antecedent'], top_rules['consequent']]).unique())\n",
        "antecedent_indices = [labels.index(a) for a in top_rules['antecedent']]\n",
        "consequent_indices = [labels.index(c) for c in top_rules['consequent']]\n",
        "\n",
        "# Create the ribbon chart\n",
        "fig = go.Figure(data=[go.Sankey(\n",
        "    node=dict(\n",
        "        pad=15,\n",
        "        thickness=20,\n",
        "        line=dict(color=\"black\", width=0.5),\n",
        "        label=labels,\n",
        "        color=\"blue\"\n",
        "    ),\n",
        "    link=dict(\n",
        "        source=antecedent_indices,  # Indices correspond to labels\n",
        "        target=consequent_indices,\n",
        "        value=top_rules['confidence'],\n",
        "        color='orange'\n",
        "    ))])\n",
        "\n",
        "fig.update_layout(title_text=\"Association Rules Ribbon Chart\", font_size=10)\n",
        "fig.show(renderer=\"colab\")"
      ],
      "metadata": {
        "id": "T9vs7KyoI8aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically, a lift value of 1 indicates that the probability of occurrence of the antecedent and consequent together are independent of each other. Hence, the idea is to look for rules having a lift much greater than 1.  So, let's see how much rules has lift greater than 1, equal 1 and less than one:"
      ],
      "metadata": {
        "id": "_xvoAft-I_xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rules_df.lift.apply(lambda x: 'Greater Than One' if x > 1 else 'One' \\\n",
        "                           if x == 0 else 'Less Than One').value_counts()"
      ],
      "metadata": {
        "id": "Kr4EjjkHJB3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_colwidth = dw"
      ],
      "metadata": {
        "id": "FfjT-av0JE1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z404aUMP2Ob2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Findings and Recommendations:**\n",
        "\n",
        "* **Customer Behavior:** The analysis revealed insights into customer purchase behavior, including top customers, purchase frequency, and sales performance of top products.\n",
        "* **RFM Segmentation:** The RFM model identified potential customer segments based on their recency, frequency, and monetary value.\n",
        "* **Clustering Potential:** The project hinted at the potential for K-means clustering to further refine customer segmentation.\n",
        "* **Marketing and Product Optimization:** The findings can be used to develop targeted marketing campaigns, optimize product placement, and identify new customer segments.\n",
        "* **Association Rule Mining:** The project suggested further analysis using association rule mining to identify product relationships for recommendations and promotions.\n",
        "\n",
        "**Considerations and Future Directions:**\n",
        "\n",
        "* **Computational Efficiency:** The generation of association rules can be computationally expensive, especially with large datasets. Balancing support and confidence levels is crucial for obtaining a reasonable number of strong rules.\n",
        "* **Rule Targeting:** For rare but high-confidence patterns, consider setting low support and high confidence levels to identify potential areas for cross-selling strategies.\n",
        "* **Clustering Evaluation:** If K-means clustering is pursued, careful evaluation of cluster quality using methods like Silhouette analysis is essential.\n",
        "* **Data Quality:** Ensure data quality and consistency throughout the analysis to avoid biases and inaccuracies in the results.\n",
        "\n",
        "**Overall, this project demonstrates the application of unsupervised machine learning techniques for customer segmentation and business decision-making in the online retail industry.** By leveraging insights from customer behavior, RFM analysis, and potential clustering, businesses can tailor their marketing strategies and product offerings to enhance customer satisfaction and drive sales.\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}